{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "os.chdir(\"data\")\n",
    "data = loadmat(\"data.mat\")\n",
    "os.chdir(\"..\")\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'             loss_history.append(self.compute_loss(X,Y))\\n            if epoch > 10 and abs(loss_history[-1] - loss_history[-2]) < 1e-7:\\n                print(f\"Early stopping at epoch {epoch}\")\\n                break\\n        return '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate =0.01, iterations = 1000, regularization=0.1, stochastic=True, divide_lr = False):\n",
    "        \"\"\"\"\n",
    "        L2 regularized logistic regression model\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.regularization = regularization\n",
    "        self.stochastic = stochastic\n",
    "        self.divide_lr = divide_lr\n",
    "        return\n",
    "    def normalize(self, X):\n",
    "        \"\"\"\n",
    "        Normalizes ny mean and standard deviation\n",
    "        \"\"\"\n",
    "        mu = np.mean(X, axis=0)\n",
    "        std = np.std(X,axis=0)\n",
    "        X_norm = (X-mu)/std\n",
    "        return X_norm\n",
    "    def add_bias(self, X):\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack((np.ones((n, 1)), X))  # Add bias term\n",
    "        return X\n",
    "    def fit(self, X, Y, w=None):\n",
    "        \"\"\"\"\n",
    "        Use w to set an inital weight vector if desired\n",
    "        \"\"\"\n",
    "        X_norm = self.normalize(X)\n",
    "        X_norm = self.add_bias(X_norm)\n",
    "        m, n = X_norm.shape\n",
    "        if w is not None:\n",
    "            self.w = np.full((n,1), w)\n",
    "        else:\n",
    "            self.w = np.zeros((n, 1))  # Initialize weights\n",
    "\n",
    "        if not self.stochastic:\n",
    "            self.batch_gradient_descent(X_norm,Y)\n",
    "        else:\n",
    "            self.stochastic_gradient_descent(X_norm,Y)\n",
    "        return\n",
    "    def compute_loss(self, X, Y, external=False):\n",
    "        \"\"\"\n",
    "        Compute mean cross-entropy loss\n",
    "        \"\"\"\n",
    "        if external:\n",
    "            X = self.normalize(X)\n",
    "            X = self.add_bias(X)\n",
    "        y_hat = expit(X @ self.w)\n",
    "        m = len(Y)\n",
    "        loss = (-1/m) * np.sum(Y * np.log(y_hat) + (1 - Y) * np.log(1 - y_hat))\n",
    "        return loss\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\"\n",
    "        returns probabilities under logistic function\n",
    "        \"\"\"\n",
    "        return expit(X @ self.w)\n",
    "    def batch_gradient_descent(self, X, Y):\n",
    "        \"\"\"\"\n",
    "        Batch Gradient descent\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(self.iterations), desc=f\"Batch GD with {self.iterations} iterations: \"):\n",
    "            y_hat = self.predict_proba(X)\n",
    "            n = X.shape[0]\n",
    "            gradient = (1/n) * X.T @ (y_hat - Y)\n",
    "            gradient += (self.regularization/n) * np.vstack(([0], self.w[1:])) \n",
    "            self.w -= self.learning_rate * gradient\n",
    "    def predict(self,X,threshold=0.5):\n",
    "        X_norm = self.normalize(X)\n",
    "        X_norm = self.add_bias(X_norm)\n",
    "        preds = (self.predict_proba(X_norm) >= threshold).astype(int)\n",
    "        return np.ravel(preds)\n",
    "    def stochastic_gradient_descent(self,X,Y):\n",
    "        lr = self.learning_rate\n",
    "        loss_history = []\n",
    "        s = seed\n",
    "        for i in tqdm(range(1, self.iterations +1), desc=f\"SGD with {self.iterations} iterations: \"):\n",
    "            s+=1\n",
    "            rng = np.random.RandomState(s)\n",
    "            inds = rng.permutation(X.shape[0])\n",
    "            X_shuffle = X[inds]\n",
    "            Y_shuffle = Y[inds]\n",
    "            for j in range(1):  #allows for an easy switch to minibatch GD\n",
    "                x_i = X_shuffle[j].reshape(1, -1)  #\n",
    "                y_i = Y_shuffle[j]\n",
    "\n",
    "                y_hat = expit(x_i @ self.w)  \n",
    "                error = y_hat - y_i\n",
    "                self.w[1:] = (1 - lr * self.regularization) * self.w[1:] - lr * error * x_i.T[1:]\n",
    "                self.w[0] -= lr * error.item() \n",
    "                if self.divide_lr:\n",
    "                    lr = self.learning_rate / i\n",
    "        return\n",
    "\"\"\"             loss_history.append(self.compute_loss(X,Y))\n",
    "            if epoch > 10 and abs(loss_history[-1] - loss_history[-2]) < 1e-7:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        return \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch GD with 1000 iterations: 100%|██████████| 1000/1000 [00:00<00:00, 2759.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       787\n",
      "         1.0       0.93      0.99      0.95       213\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.96      0.98      0.97      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = data.get('X')\n",
    "Y = data.get('y')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "model = LogisticRegression(stochastic=False)\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGD with 1000 iterations: 100%|██████████| 1000/1000 [00:00<00:00, 1312.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99       787\n",
      "         1.0       0.96      0.99      0.97       213\n",
      "\n",
      "    accuracy                           0.99      1000\n",
      "   macro avg       0.98      0.99      0.98      1000\n",
      "weighted avg       0.99      0.99      0.99      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression(stochastic=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
